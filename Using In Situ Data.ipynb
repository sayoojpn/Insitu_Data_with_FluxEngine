{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7a518ef",
   "metadata": {},
   "source": [
    "# Using Insitu Data with FluxEngine #\n",
    "\n",
    "## Introduction\n",
    "This script details how to visualise collected data and apply the FluxEngine functionalities to calculate air-sea gas flux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e35f11",
   "metadata": {},
   "source": [
    "### Load Relevant Modules\n",
    "To begin with the required Python packages are loaded (note: these are required to be installed in your environment first, else an error with appear). Also this is a good time to ensure you are running this script in the same environment where you have installed FluxEngine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0382b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from netCDF4 import Dataset\n",
    "# Install basemap-data-hires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e80327b",
   "metadata": {},
   "source": [
    "### Loading the insitu data\n",
    "Now we load the insitu data. This is very simple using the Pandas library where we can use the .read_csv() function, which can load CSV (Comma Seperated Values) or TSV (Tab Seperated Values) files. If you're using a CSV file the the 'sep' keyword should be changed to ',' but if using a TSV file then you can keep the sep='\\t'. Additionally, the 'index_col' keyword is set to 0 to define that the first column in the data is simply indexing/counting the rows (i.e it's not actual data). You can try removing this with the example data to see what happens (it may help if for some reason your data does not include and index column).\n",
    "\n",
    "Change the input file to either 'Carrick_Roads.tsv' or 'Agulhas.tsv' to load the example data, or input your own data file. We then show the first 5 rows of the data using the .head() function (and you can see the bottom 5 rows by changing this to .tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b4e599",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data file\n",
    "region_data = pd.read_csv('CarrickRoads.tsv', sep='\\t', index_col=0)\n",
    "# Show small proportion of the data\n",
    "region_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6c53d4",
   "metadata": {},
   "source": [
    "### Preparing to Plot the Recorded Data\n",
    "We want to plot a 'time series' of the data that was recorded. One way to show this is to plot 'Days since [first recording]' along the x-axis and the data along the y-axis. The cell below finds the number of days since the first measurement (technically it finds the number of seconds since the first recording and divides this by 86,400) and creates a new column in the Dataframe to show these values.\n",
    "\n",
    "Note: if your own dataset doesn't have columns for 'Year', 'Month', 'Day', etc. then the below won't work and you need to add this to your dataset. This can be done in Excel (but better to do it Pythonically if possible to prevent Excel making changes to it's own formatting), and see example datasets for the required format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff7722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the new Dataframe column and fill with a hold value\n",
    "region_data['Days_since'] = 'hold value'\n",
    "\n",
    "# Produce a datetime object for the first recording \n",
    "# - the zeros in the line below show it's the first row (index starts at zero)\n",
    "start_date = dt.datetime(region_data.loc[0,'Year'],region_data.loc[0,'Month'],region_data.loc[0,'Day'],\n",
    "                            region_data.loc[0,'Hour'],region_data.loc[0,'Minute'],region_data.loc[0,'Second'])\n",
    "\n",
    "# Loop over all rows in the Dataframe - i.e from 0 to the length of the Dataframe\n",
    "for i in range(0,len(region_data)):\n",
    "    # Get the date time object for the currently indexed recording - indexed by i\n",
    "    future_date = dt.datetime(region_data.loc[i,'Year'],region_data.loc[i,'Month'],region_data.loc[i,'Day'],\n",
    "                              region_data.loc[i,'Hour'],region_data.loc[i,'Minute'],region_data.loc[i,'Second'])\n",
    "    \n",
    "    # Find difference between current datetime and inital datetime\n",
    "    day_diff = future_date - start_date\n",
    "    \n",
    "    # Fill Dataframe column with time difference in seconds (found using .total_seconds()) \n",
    "    # divided by 86400 (proportion of days that have passed)\n",
    "    region_data.loc[i,'Days_since'] = day_diff.total_seconds()/(60*60*24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f2dd95",
   "metadata": {},
   "source": [
    "We can filter the Dataframe to show just the 'Datetime' and 'Days_since' columns. Showing the head can give an idea if the previous cell worked - although a more thorough check is advised if possible depending on Dataframe size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb24cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data to 'Datetime' and 'Days_since' columns and show first 5 rows.\n",
    "region_data[['Datetime', 'Days_since']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ea18f8",
   "metadata": {},
   "source": [
    "### Plotting the Time Series  \n",
    "\n",
    "The Matplotlib .subplots() function is ideal for this and I have used Seaborn to do the actual plotting - these two packages work well together as Seaborn is built on top of Matplotlib, and Seaborn also integrates easily with Pandas Dataframes. \n",
    "\n",
    "Producing nice looking plots with lovely axes labels and colors etc. can be fiddly, but you can refer to the documentation (and StackOverflow!) for hints and tips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf66c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a figure with 4 axes on it. Sharex=True means all axes will share the bottom axes (can help with clarity)\n",
    "fig,ax = plt.subplots(4,1, sharex=True)\n",
    "# Set figure height and width\n",
    "fig.set_figheight(15), fig.set_figwidth(15)\n",
    "# Set overall title of subplots\n",
    "fig.suptitle('Time Series Plots of Insitu Data', fontsize=20)\n",
    "\n",
    "### PLOTTING THE DATA ### (- *s indicate a plot keyword below)\n",
    "# These Seaborn commands state that we want a *lineplot*, where the *data* is coming \n",
    "# from our region_data Dataframe, and we chose the *x* & *y* columns that we want, as well\n",
    "# as the axis (*ax*) we want to plot on (indexed by 0 at the top and 3 at the bottom)\n",
    "sns.lineplot(data=region_data, x='Days_since', y='SST_C', ax=ax[0])\n",
    "sns.lineplot(data=region_data, x='Days_since', y='windspeed', ax=ax[1])\n",
    "sns.lineplot(data=region_data, x='Days_since', y='xCO2air', ax=ax[2])\n",
    "sns.lineplot(data=region_data, x='Days_since', y='fCO2', ax=ax[3])\n",
    "\n",
    "# Set x axis label\n",
    "plt.xlabel(f'Days since {region_data.loc[0,\"Datetime\"]}', fontdict={'size':15})\n",
    "\n",
    "# Set y label for each axis\n",
    "ax[0].set_ylabel('SST (celsius)', fontsize = 20) \n",
    "ax[1].set_ylabel('Windspeed (m/s)', fontsize = 20)\n",
    "ax[2].set_ylabel('xCO2 (1e-6 atm/mol)', fontsize = 20)\n",
    "ax[3].set_ylabel('fCO2 (1e-6 atm)', fontsize = 20)\n",
    "\n",
    "# Changes how axis ticks are displayed for last two axes\n",
    "# - you can comment these out with # to see the effect when removed\n",
    "ax[0].yaxis.set_major_formatter('{x:9<5.2f}')\n",
    "ax[1].yaxis.set_major_formatter('{x:9<5.2f}')\n",
    "ax[2].yaxis.set_major_formatter('{x:9<5.2f}')\n",
    "ax[3].yaxis.set_major_formatter('{x:9<5.2f}')\n",
    "\n",
    "# Set a tight layout to remove extra space around the plots\n",
    "fig.tight_layout()\n",
    "# Reduce gap between top of figure and the title\n",
    "fig.subplots_adjust(top=0.95)\n",
    "\n",
    "# Show figure!\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1d5d9b-e630-482e-a82f-2fca749f476a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Alkalinity\n",
    "\n",
    "Here we can estimate the alkalinty by using a linear equation containing salinity and SST:\n",
    "\n",
    "$Alkalinty = 2305 + 53.97(SSS - 35) + 2.74(SSS - 35)^{2} - 1.16(SST - 20) - 0.04(SST - 20)^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea84f83b-2c06-44b3-9fbc-8b7e9020bc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns in dataframe for alkalinity\n",
    "region_data['alkalinity'] = 2305 + 53.97*(region_data['salinity']-35) + 2.74*((region_data['salinity']-35)**2) - 1.16*(region_data['SST_C']-20) - 0.04*((region_data['SST_C']-20)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af53a46-b166-432f-9220-2ec9ec76abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot alaklinity - uses same commands as above but as only one plot is required we don't need to index the axes\n",
    "fig, ax = plt.subplots(1,1, figsize=(15,5))\n",
    "sns.lineplot(data=region_data, x='Days_since', y='alkalinity', ax=ax)\n",
    "plt.xlabel(f'Days since {region_data.loc[0,\"Datetime\"]}', fontdict={'size':15})\n",
    "ax.set_ylabel('Alkalinity', fontsize = 20) \n",
    "ax.yaxis.set_major_formatter('{x:9<5.2f}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4dcfc6",
   "metadata": {},
   "source": [
    "### Displaying Data on a Map\n",
    "\n",
    "Now we aim to display the recorded data on a map. There are a couple of Python packages to do this but this script used Basemap (which is part of Matplotlib - although needs to be installed additionally) due to it having a decent spatial resolution. Note: if you're familar with GIS and producing Shapefiles you could attempt to use Geopandas, but it would require additional work.\n",
    "\n",
    "First we can aquire the min/max longitude and latitude for our data to give us an idea of the region to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96751c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get min and max of longitude and latitude\n",
    "region_data.describe().loc[['min','max'],['Lon','Lat']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830f1bf5",
   "metadata": {},
   "source": [
    "The fuction below has been written to allow easy plotting of the example datasets but also enable you to plot your own. Run the cell below to initalise the function (i.e nothing will happen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b53fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coords(location):\n",
    "    if location == 'CarrickRoads':\n",
    "        lon_min = -5.2\n",
    "        lon_max = -4.9\n",
    "        lat_min = 50.1\n",
    "        lat_max = 50.25\n",
    "        return lon_min, lon_max, lat_min, lat_max\n",
    "    \n",
    "    elif location == 'Agulhas':\n",
    "        lon_min = 19.7\n",
    "        lon_max = 20.2\n",
    "        lat_min = -35.0\n",
    "        lat_max = -34.7\n",
    "        return lon_min, lon_max, lat_min, lat_max\n",
    "    \n",
    "    else:\n",
    "        lon_min = input('Enter minimum longitude (most Westerly): ')\n",
    "        lon_max = input('Enter minimum longitude (most Easterly): ')\n",
    "        lat_min = input('Enter minimum latitude (most Southerly): ')\n",
    "        lat_max = input('Enter maximum latitude (most Northerly): ')\n",
    "        print('\\n\\n')\n",
    "        if (lon_min >= lon_max) or (lat_min >= lat_max):\n",
    "            print('Check if min/max were entered in the correct order (is a min greater than a max?)')\n",
    "            return np.nan, np.nan, np.nan, np.nan\n",
    "        \n",
    "        \n",
    "        return float(lon_min), float(lon_max), float(lat_min), float(lat_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91eeb6f",
   "metadata": {},
   "source": [
    "In the cell below you can change the string in the get_coords() function. Changing it to 'Carrick Roads' or 'Agulhus' will assign the min/max latitude & longtudes to those needed for plotting these regions (you can also enter 'World' but it's there purely to play with map projections if you wish). Alternatively you can enter 'Other', in which case text boxes will appear for you to enter the min/max longitudes and latitudes for your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7204d18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change string to 'CarrickRoads', 'Agulhas' or your own region name (use '_' for spaces)\n",
    "region_name = 'CarrickRoads'\n",
    "# Performs function\n",
    "lon_min, lon_max, lat_min, lat_max = get_coords(region_name)\n",
    "\n",
    "# Note: if you're having problems with the input fields you can uncomment the line below \n",
    "# and  just enter the values instead, but also comment out the line above to avoid confusion.\n",
    "\n",
    "# lon_min, lon_max, lat_min, lat_max = __ , __ , __ , __\n",
    "\n",
    "# Print out current values\n",
    "print('Current values:')\n",
    "print(f'Longitude -> \\t min:{lon_min} \\t max:{lon_max}')\n",
    "print(f'Latitude -> \\t min:{lat_min} \\t max:{lat_max}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64f2f6d",
   "metadata": {},
   "source": [
    "Now we have the longitude and latitude bounds we can plot the map. The code below essentially 1) initialises the figure, 2) defines our map, 3) adds details to the plot, 4) adds the gridlines, and 5) plots the track from the data.\n",
    "\n",
    "The current resolution is set to 'f' (full) to get a high resolution image. If you haven't installed the basemap-data-hires package this won't work and you must change this the resolution to 'i' (intermediate) which is coarser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb89bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Intialise figure and figure size\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(20,10, forward=True)\n",
    "\n",
    "# 2) Define the map \n",
    "# Here we have a cylindrical equidistant projection bound by our chosen latitude and longitudes,\n",
    "# and a chosen resolution ('i' = intermediate)\n",
    "m = Basemap(projection='cyl',\n",
    "            llcrnrlat=lat_min,urcrnrlat=lat_max,\n",
    "            llcrnrlon=lon_min,urcrnrlon=lon_max,\n",
    "            resolution='i')\n",
    "\n",
    "# 3) Fill land masses with green colour\n",
    "m.fillcontinents(color='green')\n",
    "# Add title to plot - change as you wish\n",
    "plt.title(\"Cylindrical Equidistant Projection\", fontsize=20)\n",
    "\n",
    "# 4) Draw map gridlines - the 'split_lat' and 'split_lon' have been set to show a 0.05x0.05 degree \n",
    "# grid, which reflects that given in the ESA CCI data (covered next)\n",
    "split_lon = round((lon_max - lon_min)/0.05) + 1\n",
    "lons = np.linspace(lon_min,lon_max,split_lon)\n",
    "m.drawmeridians(lons,labels=[1,0,0,1])\n",
    "\n",
    "split_lat = round((lat_max - lat_min)/0.05) + 1\n",
    "lats = np.linspace(lat_min,lat_max,split_lat)\n",
    "m.drawparallels(lats,labels=[1,0,0,1])\n",
    "\n",
    "\n",
    "track_lon, track_lat = m(np.asarray(region_data['Lon']),np.asarray(region_data['Lat']))\n",
    "plt.scatter(track_lon,track_lat, s=10, marker='o', color='Red') \n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfbe926",
   "metadata": {},
   "source": [
    "### Adding Satelite Sea Surface Sub-skin Temperature\n",
    "To use FluxEngine we need the sea surface sub-skin temperature, which can be aquired in daily resolution from the ESA CCI datasets (found here: https://resources.marine.copernicus.eu/product-detail/SST_GLO_SST_L4_REP_OBSERVATIONS_010_024/INFORMATION). Under the 'Data Access' tab there are two options 'ESACCI-GLO-SST-L4-REP-OBS-SST' which covers 1981-2016, and 'C3S-GLO-SST-L4-REP-OBS-SST' for 2017 onwards. By far the easiest way to download the data is using FTP (File Transfer Protocol) which opens the file folders remotely on your PC. Some browsers (particularlly Safari on MacOS) don't allow FTP, but this can be worked around by using a different browser, such as Firefox. You should be able to connect using the 'guest' option when it prompts you, but if you can't try creating an account with the CMEMS (https://resources.marine.copernicus.eu/registration-form) and using your login at the prompt.\n",
    "\n",
    "The resolution of the ESA CCI data is 0.05x0.05, which results in 25,920,000 data points where most are not needed. You can use the entire dataset with FluxEngine, but it takes a long time to run and also presents problems viewing the data in Panoply. Therefore the following section of code allows us to view only the data in our desired region, and then we can manually fill this in to our dataframe. \n",
    "\n",
    "Below we first read the netCDF SST file (you can subsitute the file string with your own downloaded ESA CCI SST data when needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da49597",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_ds = Dataset('20130903120000-ESACCI-L4_GHRSST-SSTdepth-OSTIA-GLOB_CDR2.0-v02.0-fv01.0.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce61e0b8",
   "metadata": {},
   "source": [
    "Now we want to find the SST values just within region we are interested (i.e the region plotted above). The code below takes our min/max longitudes and latitudes and finds where their array position is in the ESA CCI SST data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3ea0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding SST array index for min/max longitude\n",
    "lon_min_idx = round((lon_min+180)/0.05)\n",
    "lon_max_idx = round((lon_max+180)/0.05)\n",
    "\n",
    "# Finding SST array index for min/max latitude\n",
    "lat_max_idx = round(-(lat_max-90)/0.05)\n",
    "lat_min_idx = round(-(lat_min-90)/0.05)\n",
    "\n",
    "# Print the results\n",
    "print('Indexes:')\n",
    "print(f'Rows: {lat_max_idx} -> {lat_min_idx}')\n",
    "print(f'Cols: {lon_min_idx} -> {lon_max_idx}')\n",
    "print(f'Data slice used: [{lat_max_idx}:{lat_min_idx},{lon_min_idx}:{lon_max_idx}]')\n",
    "\n",
    "# Get ESA SST values by removing the mask - fill land areas with NaN values\n",
    "sst_grid = np.ma.filled(sst_ds.variables['analysed_sst'][:],fill_value=np.nan)\n",
    "sst_grid = np.squeeze(sst_grid) # Reduces data to 2 dimensions\n",
    "sst_grid = np.flip(sst_grid, axis=0) # Flip data (ESA grid is 'upside down')\n",
    "# Reduce data to desired region using the indexes found above\n",
    "region_sst_grid = sst_grid[lat_max_idx:lat_min_idx,lon_min_idx:lon_max_idx]\n",
    "\n",
    "# Plot the region SST data and annotate values (note: annotation not advised for large data sets)\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(region_sst_grid,annot=True,fmt=\".3f\",linewidths=1,linecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1308a112",
   "metadata": {},
   "source": [
    "In order to not calculate fluxes for the entire globe we need to add the ESA SST data into our region dataset. This can be done prgrammatically or using software like Excel. The big problem with using Excel is that when you import your data as a spreadsheet, Excel will change the format of the datetime column - but we need it as yyyy-mm-dd hh:mm:ss for use with FluxEngine - and then we have to go through the convoluted (but possible) process of changing the format back. Instead there is some code below which extracts the array position of the recorded data, matches this to the region SST grid, and fills in the values for you. This code can be complex to write initially, but will likely save time and effort in the long run. Try running the code below using the example datasets, and then test your own.\n",
    "\n",
    "First we edit our region dataframe to include the 'ESA_SST' column and fill it with a holding value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47434bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ESA_SST column and fill with 'hold value'\n",
    "region_data['ESA_SST'] = 'hold value'\n",
    "# Display small section of dataframe\n",
    "region_data[['Datetime','ESA_SST']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cb0e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip the order of the latitudes (matches latitude index with SST array index)\n",
    "flip_lats = np.flip(lats,axis=0)\n",
    "\n",
    "# Loop over rows in region dataframe\n",
    "for r,row in enumerate(region_data.iterrows()):\n",
    "    lat = row[1]['Lat'] # Get latitude in row\n",
    "    lon = row[1]['Lon'] # Get longitude in row\n",
    "    \n",
    "    # Loop over increasing longitude values\n",
    "    # When our 'lon' is less than the longitude we are looping over we know the array index\n",
    "    for lx in range(1,len(lons),1):\n",
    "        if lon < lons[lx]:\n",
    "            break # Breaks out of loop to keep 'lx' = array index\n",
    "            \n",
    "    # Loop over increasing latitude values\n",
    "    # When our 'lat' is less than the latitude we are looping over we know the array index      \n",
    "    for ly in range(len(flip_lats)-2,-1,-1):\n",
    "        if lat < flip_lats[ly]:\n",
    "            break # Breaks out of loop to keep 'ly' = array index\n",
    "    \n",
    "    # In the current row we changes the ESA_SST hold value to the correct value in the SST array\n",
    "    region_data.loc[r,'ESA_SST'] = region_sst_grid[ly,lx-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18b81ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual check of top of data (early in datetime)\n",
    "region_data[['Lat','Lon','Datetime','ESA_SST']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a953077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual check of bottom of data (late in datetime)\n",
    "region_data[['Lat','Lon','Datetime','ESA_SST']].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fa0928",
   "metadata": {},
   "source": [
    "Finally we need to export our new dataframe as a file to be used in FluxEngine (change the file name to suit the dataset you are using) - be careful NOT to overwrite your orginal data just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc6f9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exports data to a TSV - automatically takes name from the one chosen previously\n",
    "region_data.to_csv(f'{region_name}_withESASST.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe158ba8",
   "metadata": {},
   "source": [
    "### Using FluxEngine\n",
    "Now we have plotted our data we can use FluxEngine to export the data to netCDF files and run FluxEngine (as in tutorial 02).\n",
    "\n",
    "Note: don't forget the -h label if you need a reminder of how the commands work\n",
    "\n",
    "Due to how terminal commands are executed this has to be filled out by hand. The commands for the example dataset have been provided (and I suggest looking through all the included commands), but you will need to write your own (or edit the examples) for your own data.\n",
    "\n",
    "When using the example datasets you will need to change the input file (first string), -ncOutPath, and the --limits commands. When using your own datasets you will need to check all command inputs, and change values where needed.\n",
    "\n",
    "(Please also check your datetime format as anything except yyyy-mm-dd hh:mm:ss may cause issues - defintely double check the format if you've used Excel at any point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8c4689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays values needed for --limit command below\n",
    "print(f'These are the values for the limit commands -> South:{lat_min}, North:{lat_max}, West:{lon_min}, East:{lon_max}')\n",
    "print(f'e.g. --limits {lat_min} {lat_max} {lon_min} {lon_max}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaa1846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We primarily use FluxEngine from the command line, but here we can import it just to check the version\n",
    "import fluxengine as fe\n",
    "print(fe.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de293d1d",
   "metadata": {},
   "source": [
    "The following cell converts our text file to netCDF for use with FluxEngine.\n",
    "If changing location to the Agulhas (or your own region) ensure to change the following:\n",
    "- Text file to read (first argument)\n",
    "- Coordinates (in order of South, North, West, East) given after --limits\n",
    "- File name to write out netCDF to given after --ncOutPath\n",
    "- If using your own data may need to change start and end times too "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c743c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to netCDF\n",
    "!fe_text2ncdf.py \"CarrickRoads_withESASST.tsv\" --limits 50.1 50.25 -5.2 -4.9 --ncOutPath \"CarrickRoads.nc\" --startTime \"2013-09-03 00:00:00\" --endTime \"2013-09-03 23:59:59\" --temporalResolution \"30 00:00\" --cols \"SST_C\" \"windspeed\" \"wind_moment2\" \"air_pressure\" \"salinity\" \"xCO2air\" \"fCO2\" \"ESA_SST\" --dateIndex 3 --latProd \"Lat\" --lonProd \"Lon\" --latResolution 0.005 --lonResolution 0.005\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3456d7ae",
   "metadata": {},
   "source": [
    "Here we load the configuration file and tell FluxEngine our start and end dates. Change config file if using a different region, and change dates if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266608c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running FluxEngine\n",
    "!fe_run.py \"CarrickRoads.conf\" -s \"2013-09-01\" -e \"2013-09-30\" -l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2237b09",
   "metadata": {},
   "source": [
    "Finally we can merge our FluxEngine output into our text file. Depending on region, you may need to change the first 3 arguments:\n",
    "- 1st: FluxEngine output file\n",
    "- 2nd: Text file to merge to\n",
    "- 3rd: New file to create with merged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81625c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending FluxEngine results\n",
    "!fe_append2insitu.py \"fe_output/carrickroads/2013/09/carrickroads_fe.nc\" \"CarrickRoads.tsv\" \"CarrickRoads_merged.tsv\" --varsToAppend \"OF\" \"OK3\" \"OSFC\" \"OIC1\" --dateIndex 3 --lonCol \"Lon\" --latCol \"Lat\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791577bb",
   "metadata": {},
   "source": [
    "Now we have run FluxEngine and combine our output with our in-situ data, we can go ahead and view/visualise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96221dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load merged data\n",
    "merged_data = pd.read_csv('CarrickRoads_merged.tsv', sep='\\t',index_col=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cdc099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View top of merged data\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a820d142",
   "metadata": {},
   "source": [
    "We need to add out 'Days_since' to this new merged dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f16263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the new Dataframe column and fill with a hold value\n",
    "merged_data['Days_since'] = 'hold value'\n",
    "\n",
    "# Produce a datetime object for the first recording \n",
    "# - the zeros in the line below show it's the first row (index starts at zero)\n",
    "start_date = dt.datetime(merged_data.loc[0,'Year'],merged_data.loc[0,'Month'],merged_data.loc[0,'Day'],\n",
    "                            merged_data.loc[0,'Hour'],merged_data.loc[0,'Minute'],merged_data.loc[0,'Second'])\n",
    "\n",
    "# Loop over all rows in the Dataframe - i.e from 0 to the length of the Dataframe\n",
    "for i in range(0,len(merged_data)):\n",
    "    # Get the date time object for the currently indexed recording - indexed by i\n",
    "    future_date = dt.datetime(merged_data.loc[i,'Year'],merged_data.loc[i,'Month'],merged_data.loc[i,'Day'],\n",
    "                              merged_data.loc[i,'Hour'],merged_data.loc[i,'Minute'],merged_data.loc[i,'Second'])\n",
    "    \n",
    "    # Find difference between current datetime and inital datetime\n",
    "    day_diff = future_date - start_date\n",
    "    \n",
    "    # Fill Dataframe column with time difference in seconds (found using .total_seconds()) \n",
    "    # divided by 86400 (proportion of days that have passed)\n",
    "    merged_data.loc[i,'Days_since'] = day_diff.total_seconds()/(60*60*24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92801b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show section of 'Days_since' column for visual check\n",
    "merged_data[['Datetime', 'Days_since']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5918720d",
   "metadata": {},
   "source": [
    "### Plot the Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e132519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a figure with 4 axes on it. Sharex=True means all axes will share the bottom axes (can help with clarity)\n",
    "fig,ax = plt.subplots(1,1, sharex=True, figsize=(20,10))\n",
    "sns.lineplot(data=merged_data, x='Days_since', y='OF [g C m-2 day-1]')\n",
    "\n",
    "# Plot features \n",
    "plt.xlabel(f'Days since {merged_data.loc[0,\"Datetime\"]}', fontdict={'size':20})\n",
    "plt.ylabel('Flux [g C m-2 day-1]', fontdict={'size':20})\n",
    "plt.tick_params(labelsize=15)\n",
    "\n",
    "# Show figure!\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a03b56e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a1b31c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
